{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cyber-bullying-detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iXb_6CHe03L",
        "colab_type": "code",
        "outputId": "66688844-3c39-4696-8d5d-8fe84d80b05d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#tokenize the word\n",
        "##import libraries\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "nltk.download('punkt')\n",
        "##tokenize words without punctuation and change to lower characters\n",
        "def tokenize_wo_puctuation(str):\n",
        "  tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "  tokenized_sent = tokenizer.tokenize(str.lower())\n",
        "  return tokenized_sent\n",
        "\n",
        "#removing the stopwords\n",
        "##import libraries\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "##remove stopwords from tokenized_sent\n",
        "def removeStopwords(tokenized_sent):\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  filtered_sent=[]\n",
        "  for w in tokenized_sent:\n",
        "    if w not in stop_words:\n",
        "      filtered_sent.append(w)\n",
        "  return filtered_sent\n",
        "\n",
        "#lemmatization\n",
        "##import libraries\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "def Lemma(filtered_sent):\n",
        "  lem = WordNetLemmatizer()\n",
        "  lemmatized_sent=[]\n",
        "  for w in filtered_sent:\n",
        "    lemmatized_sent.append(lem.lemmatize(w))\n",
        "  return lemmatized_sent\n",
        "\n",
        "\n",
        "#function for preprocessing\n",
        "def preprocessing(str):\n",
        "  tokenized_sent = tokenize_wo_puctuation(str)\n",
        "  filtered_sent = removeStopwords(tokenized_sent)\n",
        "  lemmatized_sent = Lemma(filtered_sent)\n",
        "  return lemmatized_sent"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS63FHqi7ddw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function for evaluating \n",
        "##import modules\n",
        "from sklearn.model_selection import cross_val_score\n",
        "##evaluation by f1 score with 5_cross validation and output mean value.\n",
        "def evaluation(model_name, X_test_data, y_test_data):\n",
        "  scores = cross_val_score(model_name, X_test_data, y_test_data, scoring=\"f1_macro\")\n",
        "  return np.mean(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMyGucjDxk9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the train data\n",
        "##import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "##load the data\n",
        "df = pd.read_csv(\"/train_data.csv\")\n",
        "df[\"Text Label\"] = df[\"Text Label\"].map({'Bullying': 1, 'Non-Bullying': 0}) #convert \"Bullying\" into 1, \"Non-Bullying\" into 0\n",
        "df.head()\n",
        "\n",
        "##store to each data\n",
        "#X_train = df[\"Tweet\"].values\n",
        "#y_train = df[\"Text Label\"].values\n",
        "X_train,X_test,y_train,y_test = train_test_split(df[\"Tweet\"].values, df[\"Text Label\"].values, test_size=.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR5sMxr71FBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bag-of-words\n",
        "##import libraries\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "##create vector. The # of {Rows=# of document in the whole document collection}, {Columns=# of unique tokens in the whole document collection}\n",
        "bow_vect = CountVectorizer(lowercase=False, tokenizer=preprocessing)\n",
        "bow_train = bow_vect.fit_transform(X_train)\n",
        "bow_test = bow_vect.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mfy2B8e5Gjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf-idf\n",
        "##import libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "##create vector\n",
        "tfidf_vect = TfidfVectorizer(lowercase=False, tokenizer=preprocessing)\n",
        "tfidf_train = tfidf_vect.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vect.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIn-UUWMjf5f",
        "colab_type": "code",
        "outputId": "4b171c7e-cc86-4678-eb3d-812068e5ac8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Naive Bayes Classifier with bow\n",
        "##import library\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "##train the model\n",
        "bow_nb = MultinomialNB()\n",
        "bow_nb.fit(bow_train, y_train)\n",
        "\n",
        "\n",
        "#Naive Bayes Classifier with tf-idf\n",
        "##train the model\n",
        "tfidf_nb = MultinomialNB()\n",
        "tfidf_nb.fit(tfidf_train, y_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igWGyYNb46x3",
        "colab_type": "code",
        "outputId": "5f4c12c1-d7f2-4eab-f7ce-f21f44c8badb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Logistic Regression with bow\n",
        "##import library\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "##train the model\n",
        "bow_lr = LogisticRegression()\n",
        "bow_lr.fit(bow_train, y_train)\n",
        "\n",
        "\n",
        "#Logistic Regression with tf-idf\n",
        "##import library\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "##train the model\n",
        "tfidf_lr = LogisticRegression()\n",
        "tfidf_lr.fit(tfidf_train, y_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfTNPFOWxtev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bag-of-words with 2-grams\n",
        "##create vector. The # of {Rows=# of document in the whole document collection}, {Columns=# of unique tokens in the whole document collection}\n",
        "bow_bi_vect = CountVectorizer(lowercase=False, tokenizer=preprocessing, ngram_range=(2,2))\n",
        "bow_bi_train = bow_bi_vect.fit_transform(X_train)\n",
        "bow_bi_test = bow_bi_vect.transform(X_test)\n",
        "\n",
        "\n",
        "#tf-idf with 2-grams\n",
        "##create vector\n",
        "tfidf_bi_vect = TfidfVectorizer(lowercase=False, tokenizer=preprocessing, ngram_range=(2,2))\n",
        "tfidf_bi_train = tfidf_bi_vect.fit_transform(X_train)\n",
        "tfidf_bi_test = tfidf_bi_vect.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XD4VyK10D_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccf777b1-713f-4044-d555-8761b8c5fcca"
      },
      "source": [
        "#Naive Bayes Classifier with bow of 2-grams\n",
        "##train the model\n",
        "bow_bi_nb = MultinomialNB()\n",
        "bow_bi_nb.fit(bow_bi_train, y_train)\n",
        "\n",
        "\n",
        "#Naive Bayes Classifier with tf-idf of 2-grams\n",
        "##train the model\n",
        "tfidf_bi_nb = MultinomialNB()\n",
        "tfidf_bi_nb.fit(tfidf_bi_train, y_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeAro6Vn_oq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "715e2b49-ae29-4986-decb-89fce91dda5f"
      },
      "source": [
        "#Logistic Regression with bow of 2_grams\n",
        "##train the model\n",
        "bow_bi_lr = LogisticRegression()\n",
        "bow_bi_lr.fit(bow_bi_train, y_train)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_QFqQDL7Q3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the function to create the list of tweets\n",
        "def Tweets_lis(tweets):\n",
        "  labelized = []\n",
        "  for i in range(tweets.shape[0]):\n",
        "    labelized.append(preprocessing(tweets[i]))\n",
        "  return labelized\n",
        "\n",
        "X_train_lis = Tweets_lis(X_train)\n",
        "X_test_lis = Tweets_lis(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOQ1Z4TA6JdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#build the Word2Vec model\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "w2v = Word2Vec(X_train_lis, size=100, window=5, min_count=2, sg=1, hs=0) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTLhCyFPF9EB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#take mean of all the word vecotrs present in the tweets. The length will be 200.\n",
        "def word_vector(tokens, size):\n",
        "  vec = np.zeros(size).reshape((1, size))\n",
        "  c = 0\n",
        "  for word in tokens:\n",
        "    try:\n",
        "      vec += w2v[word].reshape((1,size))\n",
        "      c += 1\n",
        "    except KeyError:\n",
        "      continue\n",
        "    if c != 0:\n",
        "      vec /= c\n",
        "    return vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tvpEtiIIssW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import module\n",
        "import numpy as np\n",
        "##create the new word2vec whose shape is (# of itself, 200).\n",
        "def createWord2veclis(X_train_lis):\n",
        "  wordvec_arrays = np.zeros((len(X_train_lis), 100))\n",
        "  for i in range(len(X_train_lis)):\n",
        "    wordvec_arrays[i,:] = word_vector(X_train_lis[i], 100)\n",
        "  wordvec_df = pd.DataFrame(wordvec_arrays)\n",
        "  return wordvec_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIWx0c1ALC8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the function to remove the function which includes Nan. Also, create new y value for word2vec.\n",
        "def removeNanRow(X_data, y_data):\n",
        "  y_arrays = np.zeros((len(y_data), 1))\n",
        "  for i in range(len(y_data)):\n",
        "    y_arrays[i,0] = y_data[i]\n",
        "\n",
        "  y_data = pd.DataFrame(y_arrays)\n",
        "\n",
        "  Index = X_data[X_data.isnull().any(axis=1)].index\n",
        "  X_data=X_data.drop(Index)\n",
        "  y_data=y_data.drop(Index)\n",
        "  return X_data, y_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "log_zev0RBNn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e0e6f3c1-91b1-4378-ecc6-86710565df25"
      },
      "source": [
        "#create word2vec train and test \n",
        "wordvec_df_train = createWord2veclis(X_train_lis)\n",
        "wordvec_df_test = createWord2veclis(X_test_lis)\n",
        "w2v_x_train, w2v_y_train = removeNanRow(wordvec_df_train, y_train)\n",
        "w2v_x_test, w2v_y_test = removeNanRow(wordvec_df_test, y_test)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLtKrsyy_00e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "64ec0082-efe6-421f-faff-955af21185b9"
      },
      "source": [
        "#Logistic Regression with Word2Vec\n",
        "##train the model\n",
        "w2v_lr = LogisticRegression()\n",
        "w2v_lr.fit(w2v_x_train, w2v_y_train)\n",
        "#w2v_lr.score(w2v_x_test, w2v_y_test)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGAnHszI9OpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "66feb60d-6f84-4d58-8f79-ef4d95ecab4c"
      },
      "source": [
        "#Support Vector Machine with bow\n",
        "##import the module\n",
        "from sklearn.svm import SVC\n",
        "##train the model\n",
        "bow_SVM = SVC()\n",
        "bow_SVM.fit(bow_train, y_train)\n",
        "\n",
        "\n",
        "#Support Vector Machine with tfidf\n",
        "##train the model\n",
        "tfidf_SVM = SVC()\n",
        "tfidf_SVM.fit(tfidf_train, y_train)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2OVmqpwicfx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "ec5fc6ad-f156-40bf-a32f-8bb4bbb28935"
      },
      "source": [
        "#nerural network with bow\n",
        "##import the module\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "##train the model\n",
        "bow_mlp = MLPClassifier()\n",
        "bow_mlp.fit(bow_train, y_train)\n",
        "\n",
        "#nerural network with tf-idf\n",
        "tfidf_mlp = MLPClassifier()\n",
        "tfidf_mlp.fit(tfidf_train, y_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHP8tWsOl3gC",
        "colab_type": "code",
        "outputId": "a8fe0dbb-205d-4722-bdd6-0533e7bb048c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "print(\"Naive Bayes Classifier with bow(Bag-of-words): \", evaluation(bow_nb, bow_test, y_test))\n",
        "print(\"Naive Bayes Classifier with tf-idf(term frequency and inverse document frequency):\", evaluation(tfidf_nb, tfidf_test, y_test))\n",
        "print(\"Logistic Regression with bow(Bag-of-words): \", evaluation(bow_lr, bow_test, y_test))\n",
        "print(\"Logistic Regression with tf-idf(term frequency and inverse document frequency): \", evaluation(tfidf_lr, tfidf_test, y_test))\n",
        "\n",
        "print(\"Naive Bayes Classifier with bow(Bag-of-words with bigrams): \", evaluation(bow_bi_nb, bow_bi_test, y_test))\n",
        "print(\"Logistic Regression with bow(Bag-of-words with bigrams): \", evaluation(bow_bi_lr, bow_bi_test, y_test))\n",
        "print(\"Logistic Regression with word2vec: \", evaluation(w2v_lr, w2v_x_test, w2v_y_test))\n",
        "\n",
        "print(\"Support Vector Machine with bow: \", evaluation(bow_SVM, bow_test, y_test))\n",
        "print(\"Support Vector Machine with tf-idf \", evaluation(tfidf_SVM, tfidf_test, y_test))\n",
        "\n",
        "print(\"Neural Network with bow: \", evaluation(bow_mlp, bow_test, y_test))\n",
        "print(\"Neural Network with tf-idf \", evaluation(tfidf_mlp, tfidf_test, y_test))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes Classifier with bow(Bag-of-words):  0.7135005665788313\n",
            "Naive Bayes Classifier with tf-idf(term frequency and inverse document frequency): 0.6477965688465223\n",
            "Logistic Regression with bow(Bag-of-words):  0.7252194851639914\n",
            "Logistic Regression with tf-idf(term frequency and inverse document frequency):  0.6301903404527293\n",
            "Naive Bayes Classifier with bow(Bag-of-words with bigrams):  0.4610397241647243\n",
            "Logistic Regression with bow(Bag-of-words with bigrams):  0.4610397241647243\n",
            "Logistic Regression with word2vec:  0.35471871412169925\n",
            "Support Vector Machine with bow:  0.6221553460751836\n",
            "Support Vector Machine with tf-idf  0.567646867096558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Neural Network with bow:  0.6324933614528879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Neural Network with tf-idf  0.6548685646801149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqM02zlX0ALt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use 3 models, Logistic Regression with bow, SVM with bow, Neural Network with bow.\n",
        "#This is the final model\n",
        "def detection_machine(inp):\n",
        "  InputLis = []\n",
        "  InputLis.append(inp)\n",
        "  InputVect = bow_vect.transform(InputLis)\n",
        "\n",
        "  lr_result = bow_lr.predict(InputVect)\n",
        "  svm_result = bow_SVM.predict(InputVect)\n",
        "  nn_result = bow_mlp.predict(InputVect)\n",
        "  \n",
        "  sum = lr_result+svm_result+nn_result\n",
        "\n",
        "  if(sum >= 2):\n",
        "    print(\"Bullying\")\n",
        "  else:\n",
        "    print(\"Non-Bullying\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPja7B6YAvUW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "2fd49cb4-0b5f-44ea-edd2-c877e7b91a22"
      },
      "source": [
        "#trial\n",
        "b1 = \"Oh, please shut the fuck up Alecia\"\n",
        "b2 = \"Fuck Justin bieber. He's a fucking UM excuse yuh? HE IS NOT!! Ur a fuckin idiot! Kay? good\"\n",
        "b3 = \"Hey. Why you such a bitch?\"\n",
        "b4 = \"Now how the fuck am i suppose to remember that? Weirdo! maybe it was ur moms dick? lOL!\"\n",
        "b5 = \"who decided to take a shit on your face?\"\n",
        "\n",
        "n1 = \"Who do you think should be the next president of the United States?\"\n",
        "n2 = \"Who is the funniest person you know?\"\n",
        "n3 = \"would u ever go bungee jumping like justin did?\"\n",
        "\n",
        "print(detection_machine(b1))\n",
        "print(detection_machine(b2))\n",
        "print(detection_machine(b3))\n",
        "print(detection_machine(b4))\n",
        "print(detection_machine(b5))\n",
        "print(detection_machine(n1))\n",
        "print(detection_machine(n2))\n",
        "print(detection_machine(n3))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bullying\n",
            "None\n",
            "Bullying\n",
            "None\n",
            "Bullying\n",
            "None\n",
            "Bullying\n",
            "None\n",
            "Bullying\n",
            "None\n",
            "Non-Bullying\n",
            "None\n",
            "Non-Bullying\n",
            "None\n",
            "Non-Bullying\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}